// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"fmt"
	"github.com/kong/terraform-provider-konnect/v2/internal/sdk/internal/utils"
	"github.com/kong/terraform-provider-konnect/v2/internal/sdk/types"
)

type AiProxyPluginAfter struct {
	Access []string `json:"access,omitempty"`
}

func (o *AiProxyPluginAfter) GetAccess() []string {
	if o == nil {
		return nil
	}
	return o.Access
}

type AiProxyPluginBefore struct {
	Access []string `json:"access,omitempty"`
}

func (o *AiProxyPluginBefore) GetAccess() []string {
	if o == nil {
		return nil
	}
	return o.Access
}

type AiProxyPluginOrdering struct {
	After  *AiProxyPluginAfter  `json:"after,omitempty"`
	Before *AiProxyPluginBefore `json:"before,omitempty"`
}

func (o *AiProxyPluginOrdering) GetAfter() *AiProxyPluginAfter {
	if o == nil {
		return nil
	}
	return o.After
}

func (o *AiProxyPluginOrdering) GetBefore() *AiProxyPluginBefore {
	if o == nil {
		return nil
	}
	return o.Before
}

type AiProxyPluginPartials struct {
	// A string representing a UUID (universally unique identifier).
	ID *string `json:"id,omitempty"`
	// A unique string representing a UTF-8 encoded name.
	Name *string `default:"null" json:"name"`
	Path *string `default:"null" json:"path"`
}

func (a AiProxyPluginPartials) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AiProxyPluginPartials) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AiProxyPluginPartials) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *AiProxyPluginPartials) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *AiProxyPluginPartials) GetPath() *string {
	if o == nil {
		return nil
	}
	return o.Path
}

// ParamLocation - Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.
type ParamLocation string

const (
	ParamLocationBody  ParamLocation = "body"
	ParamLocationQuery ParamLocation = "query"
)

func (e ParamLocation) ToPointer() *ParamLocation {
	return &e
}
func (e *ParamLocation) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "body":
		fallthrough
	case "query":
		*e = ParamLocation(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParamLocation: %v", v)
	}
}

type Auth struct {
	// If enabled, the authorization header or parameter can be overridden in the request by the value configured in the plugin.
	AllowOverride *bool `default:"false" json:"allow_override"`
	// Set this if you are using an AWS provider (Bedrock) and you are authenticating using static IAM User credentials. Setting this will override the AWS_ACCESS_KEY_ID environment variable for this plugin instance.
	AwsAccessKeyID *string `default:"null" json:"aws_access_key_id"`
	// Set this if you are using an AWS provider (Bedrock) and you are authenticating using static IAM User credentials. Setting this will override the AWS_SECRET_ACCESS_KEY environment variable for this plugin instance.
	AwsSecretAccessKey *string `default:"null" json:"aws_secret_access_key"`
	// If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client ID.
	AzureClientID *string `default:"null" json:"azure_client_id"`
	// If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client secret.
	AzureClientSecret *string `default:"null" json:"azure_client_secret"`
	// If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the tenant ID.
	AzureTenantID *string `default:"null" json:"azure_tenant_id"`
	// Set true to use the Azure Cloud Managed Identity (or user-assigned identity) to authenticate with Azure-provider models.
	AzureUseManagedIdentity *bool `default:"false" json:"azure_use_managed_identity"`
	// Set this field to the full JSON of the GCP service account to authenticate, if required. If null (and gcp_use_service_account is true), Kong will attempt to read from environment variable `GCP_SERVICE_ACCOUNT`.
	GcpServiceAccountJSON *string `default:"null" json:"gcp_service_account_json"`
	// Use service account auth for GCP-based providers and models.
	GcpUseServiceAccount *bool `default:"false" json:"gcp_use_service_account"`
	// If AI model requires authentication via Authorization or API key header, specify its name here.
	HeaderName *string `default:"null" json:"header_name"`
	// Specify the full auth header value for 'header_name', for example 'Bearer key' or just 'key'.
	HeaderValue *string `default:"null" json:"header_value"`
	// Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.
	ParamLocation *ParamLocation `json:"param_location,omitempty"`
	// If AI model requires authentication via query parameter, specify its name here.
	ParamName *string `default:"null" json:"param_name"`
	// Specify the full parameter value for 'param_name'.
	ParamValue *string `default:"null" json:"param_value"`
}

func (a Auth) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *Auth) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Auth) GetAllowOverride() *bool {
	if o == nil {
		return nil
	}
	return o.AllowOverride
}

func (o *Auth) GetAwsAccessKeyID() *string {
	if o == nil {
		return nil
	}
	return o.AwsAccessKeyID
}

func (o *Auth) GetAwsSecretAccessKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretAccessKey
}

func (o *Auth) GetAzureClientID() *string {
	if o == nil {
		return nil
	}
	return o.AzureClientID
}

func (o *Auth) GetAzureClientSecret() *string {
	if o == nil {
		return nil
	}
	return o.AzureClientSecret
}

func (o *Auth) GetAzureTenantID() *string {
	if o == nil {
		return nil
	}
	return o.AzureTenantID
}

func (o *Auth) GetAzureUseManagedIdentity() *bool {
	if o == nil {
		return nil
	}
	return o.AzureUseManagedIdentity
}

func (o *Auth) GetGcpServiceAccountJSON() *string {
	if o == nil {
		return nil
	}
	return o.GcpServiceAccountJSON
}

func (o *Auth) GetGcpUseServiceAccount() *bool {
	if o == nil {
		return nil
	}
	return o.GcpUseServiceAccount
}

func (o *Auth) GetHeaderName() *string {
	if o == nil {
		return nil
	}
	return o.HeaderName
}

func (o *Auth) GetHeaderValue() *string {
	if o == nil {
		return nil
	}
	return o.HeaderValue
}

func (o *Auth) GetParamLocation() *ParamLocation {
	if o == nil {
		return nil
	}
	return o.ParamLocation
}

func (o *Auth) GetParamName() *string {
	if o == nil {
		return nil
	}
	return o.ParamName
}

func (o *Auth) GetParamValue() *string {
	if o == nil {
		return nil
	}
	return o.ParamValue
}

// AiProxyPluginGenaiCategory - Generative AI category of the request
type AiProxyPluginGenaiCategory string

const (
	AiProxyPluginGenaiCategoryAudioSpeech        AiProxyPluginGenaiCategory = "audio/speech"
	AiProxyPluginGenaiCategoryAudioTranscription AiProxyPluginGenaiCategory = "audio/transcription"
	AiProxyPluginGenaiCategoryImageGeneration    AiProxyPluginGenaiCategory = "image/generation"
	AiProxyPluginGenaiCategoryTextEmbeddings     AiProxyPluginGenaiCategory = "text/embeddings"
	AiProxyPluginGenaiCategoryTextGeneration     AiProxyPluginGenaiCategory = "text/generation"
)

func (e AiProxyPluginGenaiCategory) ToPointer() *AiProxyPluginGenaiCategory {
	return &e
}
func (e *AiProxyPluginGenaiCategory) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "audio/speech":
		fallthrough
	case "audio/transcription":
		fallthrough
	case "image/generation":
		fallthrough
	case "text/embeddings":
		fallthrough
	case "text/generation":
		*e = AiProxyPluginGenaiCategory(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AiProxyPluginGenaiCategory: %v", v)
	}
}

// AiProxyPluginLlmFormat - LLM input and output format and schema to use
type AiProxyPluginLlmFormat string

const (
	AiProxyPluginLlmFormatBedrock     AiProxyPluginLlmFormat = "bedrock"
	AiProxyPluginLlmFormatCohere      AiProxyPluginLlmFormat = "cohere"
	AiProxyPluginLlmFormatGemini      AiProxyPluginLlmFormat = "gemini"
	AiProxyPluginLlmFormatHuggingface AiProxyPluginLlmFormat = "huggingface"
	AiProxyPluginLlmFormatOpenai      AiProxyPluginLlmFormat = "openai"
)

func (e AiProxyPluginLlmFormat) ToPointer() *AiProxyPluginLlmFormat {
	return &e
}
func (e *AiProxyPluginLlmFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "bedrock":
		fallthrough
	case "cohere":
		fallthrough
	case "gemini":
		fallthrough
	case "huggingface":
		fallthrough
	case "openai":
		*e = AiProxyPluginLlmFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AiProxyPluginLlmFormat: %v", v)
	}
}

type Logging struct {
	// If enabled, will log the request and response body into the Kong log plugin(s) output.
	LogPayloads *bool `default:"false" json:"log_payloads"`
	// If enabled and supported by the driver, will add model usage and token metrics into the Kong log plugin(s) output.
	LogStatistics *bool `default:"false" json:"log_statistics"`
}

func (l Logging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *Logging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Logging) GetLogPayloads() *bool {
	if o == nil {
		return nil
	}
	return o.LogPayloads
}

func (o *Logging) GetLogStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.LogStatistics
}

type Bedrock struct {
	// If using AWS providers (Bedrock) you can assume a different role after authentication with the current IAM context is successful.
	AwsAssumeRoleArn *string `default:"null" json:"aws_assume_role_arn"`
	// If using AWS providers (Bedrock) you can override the `AWS_REGION` environment variable by setting this option.
	AwsRegion *string `default:"null" json:"aws_region"`
	// If using AWS providers (Bedrock), set the identifier of the assumed role session.
	AwsRoleSessionName *string `default:"null" json:"aws_role_session_name"`
	// If using AWS providers (Bedrock), override the STS endpoint URL when assuming a different role.
	AwsStsEndpointURL *string `default:"null" json:"aws_sts_endpoint_url"`
	// If using AWS providers (Bedrock), set to true to normalize the embeddings.
	EmbeddingsNormalize *bool `default:"false" json:"embeddings_normalize"`
	// Force the client's performance configuration 'latency' for all requests. Leave empty to let the consumer select the performance configuration.
	PerformanceConfigLatency *string `default:"null" json:"performance_config_latency"`
}

func (b Bedrock) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(b, "", false)
}

func (b *Bedrock) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &b, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Bedrock) GetAwsAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AwsAssumeRoleArn
}

func (o *Bedrock) GetAwsRegion() *string {
	if o == nil {
		return nil
	}
	return o.AwsRegion
}

func (o *Bedrock) GetAwsRoleSessionName() *string {
	if o == nil {
		return nil
	}
	return o.AwsRoleSessionName
}

func (o *Bedrock) GetAwsStsEndpointURL() *string {
	if o == nil {
		return nil
	}
	return o.AwsStsEndpointURL
}

func (o *Bedrock) GetEmbeddingsNormalize() *bool {
	if o == nil {
		return nil
	}
	return o.EmbeddingsNormalize
}

func (o *Bedrock) GetPerformanceConfigLatency() *string {
	if o == nil {
		return nil
	}
	return o.PerformanceConfigLatency
}

// EmbeddingInputType - The purpose of the input text to calculate embedding vectors.
type EmbeddingInputType string

const (
	EmbeddingInputTypeClassification EmbeddingInputType = "classification"
	EmbeddingInputTypeClustering     EmbeddingInputType = "clustering"
	EmbeddingInputTypeImage          EmbeddingInputType = "image"
	EmbeddingInputTypeSearchDocument EmbeddingInputType = "search_document"
	EmbeddingInputTypeSearchQuery    EmbeddingInputType = "search_query"
)

func (e EmbeddingInputType) ToPointer() *EmbeddingInputType {
	return &e
}
func (e *EmbeddingInputType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "classification":
		fallthrough
	case "clustering":
		fallthrough
	case "image":
		fallthrough
	case "search_document":
		fallthrough
	case "search_query":
		*e = EmbeddingInputType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for EmbeddingInputType: %v", v)
	}
}

type Cohere struct {
	// The purpose of the input text to calculate embedding vectors.
	EmbeddingInputType *EmbeddingInputType `default:"classification" json:"embedding_input_type"`
	// Wait for the model if it is not ready
	WaitForModel *bool `default:"null" json:"wait_for_model"`
}

func (c Cohere) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *Cohere) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Cohere) GetEmbeddingInputType() *EmbeddingInputType {
	if o == nil {
		return nil
	}
	return o.EmbeddingInputType
}

func (o *Cohere) GetWaitForModel() *bool {
	if o == nil {
		return nil
	}
	return o.WaitForModel
}

type Gemini struct {
	// If running Gemini on Vertex, specify the regional API endpoint (hostname only).
	APIEndpoint *string `default:"null" json:"api_endpoint"`
	// If running Gemini on Vertex, specify the location ID.
	LocationID *string `default:"null" json:"location_id"`
	// If running Gemini on Vertex, specify the project ID.
	ProjectID *string `default:"null" json:"project_id"`
}

func (g Gemini) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(g, "", false)
}

func (g *Gemini) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &g, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Gemini) GetAPIEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.APIEndpoint
}

func (o *Gemini) GetLocationID() *string {
	if o == nil {
		return nil
	}
	return o.LocationID
}

func (o *Gemini) GetProjectID() *string {
	if o == nil {
		return nil
	}
	return o.ProjectID
}

type Huggingface struct {
	// Use the cache layer on the inference API
	UseCache *bool `default:"null" json:"use_cache"`
	// Wait for the model if it is not ready
	WaitForModel *bool `default:"null" json:"wait_for_model"`
}

func (h Huggingface) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *Huggingface) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Huggingface) GetUseCache() *bool {
	if o == nil {
		return nil
	}
	return o.UseCache
}

func (o *Huggingface) GetWaitForModel() *bool {
	if o == nil {
		return nil
	}
	return o.WaitForModel
}

// Llama2Format - If using llama2 provider, select the upstream message format.
type Llama2Format string

const (
	Llama2FormatOllama Llama2Format = "ollama"
	Llama2FormatOpenai Llama2Format = "openai"
	Llama2FormatRaw    Llama2Format = "raw"
)

func (e Llama2Format) ToPointer() *Llama2Format {
	return &e
}
func (e *Llama2Format) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ollama":
		fallthrough
	case "openai":
		fallthrough
	case "raw":
		*e = Llama2Format(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Llama2Format: %v", v)
	}
}

// MistralFormat - If using mistral provider, select the upstream message format.
type MistralFormat string

const (
	MistralFormatOllama MistralFormat = "ollama"
	MistralFormatOpenai MistralFormat = "openai"
)

func (e MistralFormat) ToPointer() *MistralFormat {
	return &e
}
func (e *MistralFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ollama":
		fallthrough
	case "openai":
		*e = MistralFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MistralFormat: %v", v)
	}
}

// OptionsObj - Key/value settings for the model
type OptionsObj struct {
	// Defines the schema/API version, if using Anthropic provider.
	AnthropicVersion *string `default:"null" json:"anthropic_version"`
	// 'api-version' for Azure OpenAI instances.
	AzureAPIVersion *string `default:"2023-05-15" json:"azure_api_version"`
	// Deployment ID for Azure OpenAI instances.
	AzureDeploymentID *string `default:"null" json:"azure_deployment_id"`
	// Instance name for Azure OpenAI hosted models.
	AzureInstance *string  `default:"null" json:"azure_instance"`
	Bedrock       *Bedrock `json:"bedrock,omitempty"`
	Cohere        *Cohere  `json:"cohere,omitempty"`
	// If using embeddings models, set the number of dimensions to generate.
	EmbeddingsDimensions *int64       `default:"null" json:"embeddings_dimensions"`
	Gemini               *Gemini      `json:"gemini,omitempty"`
	Huggingface          *Huggingface `json:"huggingface,omitempty"`
	// Defines the cost per 1M tokens in your prompt.
	InputCost *float64 `default:"null" json:"input_cost"`
	// If using llama2 provider, select the upstream message format.
	Llama2Format *Llama2Format `json:"llama2_format,omitempty"`
	// Defines the max_tokens, if using chat or completion models.
	MaxTokens *int64 `default:"null" json:"max_tokens"`
	// If using mistral provider, select the upstream message format.
	MistralFormat *MistralFormat `json:"mistral_format,omitempty"`
	// Defines the cost per 1M tokens in the output of the AI.
	OutputCost *float64 `default:"null" json:"output_cost"`
	// Defines the matching temperature, if using chat or completion models.
	Temperature *float64 `default:"null" json:"temperature"`
	// Defines the top-k most likely tokens, if supported.
	TopK *int64 `default:"null" json:"top_k"`
	// Defines the top-p probability mass, if supported.
	TopP *float64 `default:"null" json:"top_p"`
	// Manually specify or override the AI operation path, used when e.g. using the 'preserve' route_type.
	UpstreamPath *string `default:"null" json:"upstream_path"`
	// Manually specify or override the full URL to the AI operation endpoints, when calling (self-)hosted models, or for running via a private endpoint.
	UpstreamURL *string `default:"null" json:"upstream_url"`
}

func (o OptionsObj) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OptionsObj) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OptionsObj) GetAnthropicVersion() *string {
	if o == nil {
		return nil
	}
	return o.AnthropicVersion
}

func (o *OptionsObj) GetAzureAPIVersion() *string {
	if o == nil {
		return nil
	}
	return o.AzureAPIVersion
}

func (o *OptionsObj) GetAzureDeploymentID() *string {
	if o == nil {
		return nil
	}
	return o.AzureDeploymentID
}

func (o *OptionsObj) GetAzureInstance() *string {
	if o == nil {
		return nil
	}
	return o.AzureInstance
}

func (o *OptionsObj) GetBedrock() *Bedrock {
	if o == nil {
		return nil
	}
	return o.Bedrock
}

func (o *OptionsObj) GetCohere() *Cohere {
	if o == nil {
		return nil
	}
	return o.Cohere
}

func (o *OptionsObj) GetEmbeddingsDimensions() *int64 {
	if o == nil {
		return nil
	}
	return o.EmbeddingsDimensions
}

func (o *OptionsObj) GetGemini() *Gemini {
	if o == nil {
		return nil
	}
	return o.Gemini
}

func (o *OptionsObj) GetHuggingface() *Huggingface {
	if o == nil {
		return nil
	}
	return o.Huggingface
}

func (o *OptionsObj) GetInputCost() *float64 {
	if o == nil {
		return nil
	}
	return o.InputCost
}

func (o *OptionsObj) GetLlama2Format() *Llama2Format {
	if o == nil {
		return nil
	}
	return o.Llama2Format
}

func (o *OptionsObj) GetMaxTokens() *int64 {
	if o == nil {
		return nil
	}
	return o.MaxTokens
}

func (o *OptionsObj) GetMistralFormat() *MistralFormat {
	if o == nil {
		return nil
	}
	return o.MistralFormat
}

func (o *OptionsObj) GetOutputCost() *float64 {
	if o == nil {
		return nil
	}
	return o.OutputCost
}

func (o *OptionsObj) GetTemperature() *float64 {
	if o == nil {
		return nil
	}
	return o.Temperature
}

func (o *OptionsObj) GetTopK() *int64 {
	if o == nil {
		return nil
	}
	return o.TopK
}

func (o *OptionsObj) GetTopP() *float64 {
	if o == nil {
		return nil
	}
	return o.TopP
}

func (o *OptionsObj) GetUpstreamPath() *string {
	if o == nil {
		return nil
	}
	return o.UpstreamPath
}

func (o *OptionsObj) GetUpstreamURL() *string {
	if o == nil {
		return nil
	}
	return o.UpstreamURL
}

// Provider - AI provider request format - Kong translates requests to and from the specified backend compatible formats.
type Provider string

const (
	ProviderAnthropic   Provider = "anthropic"
	ProviderAzure       Provider = "azure"
	ProviderBedrock     Provider = "bedrock"
	ProviderCohere      Provider = "cohere"
	ProviderGemini      Provider = "gemini"
	ProviderHuggingface Provider = "huggingface"
	ProviderLlama2      Provider = "llama2"
	ProviderMistral     Provider = "mistral"
	ProviderOpenai      Provider = "openai"
)

func (e Provider) ToPointer() *Provider {
	return &e
}
func (e *Provider) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "anthropic":
		fallthrough
	case "azure":
		fallthrough
	case "bedrock":
		fallthrough
	case "cohere":
		fallthrough
	case "gemini":
		fallthrough
	case "huggingface":
		fallthrough
	case "llama2":
		fallthrough
	case "mistral":
		fallthrough
	case "openai":
		*e = Provider(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Provider: %v", v)
	}
}

type Model struct {
	// Model name to execute.
	Name *string `default:"null" json:"name"`
	// Key/value settings for the model
	Options *OptionsObj `json:"options,omitempty"`
	// AI provider request format - Kong translates requests to and from the specified backend compatible formats.
	Provider Provider `json:"provider"`
}

func (m Model) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *Model) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Model) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *Model) GetOptions() *OptionsObj {
	if o == nil {
		return nil
	}
	return o.Options
}

func (o *Model) GetProvider() Provider {
	if o == nil {
		return Provider("")
	}
	return o.Provider
}

// ResponseStreaming - Whether to 'optionally allow', 'deny', or 'always' (force) the streaming of answers via server sent events.
type ResponseStreaming string

const (
	ResponseStreamingAllow  ResponseStreaming = "allow"
	ResponseStreamingAlways ResponseStreaming = "always"
	ResponseStreamingDeny   ResponseStreaming = "deny"
)

func (e ResponseStreaming) ToPointer() *ResponseStreaming {
	return &e
}
func (e *ResponseStreaming) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "allow":
		fallthrough
	case "always":
		fallthrough
	case "deny":
		*e = ResponseStreaming(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ResponseStreaming: %v", v)
	}
}

// RouteType - The model's operation implementation, for this provider.
type RouteType string

const (
	RouteTypeAudioV1AudioSpeech         RouteType = "audio/v1/audio/speech"
	RouteTypeAudioV1AudioTranscriptions RouteType = "audio/v1/audio/transcriptions"
	RouteTypeAudioV1AudioTranslations   RouteType = "audio/v1/audio/translations"
	RouteTypeImageV1ImagesEdits         RouteType = "image/v1/images/edits"
	RouteTypeImageV1ImagesGenerations   RouteType = "image/v1/images/generations"
	RouteTypeLlmV1Assistants            RouteType = "llm/v1/assistants"
	RouteTypeLlmV1Batches               RouteType = "llm/v1/batches"
	RouteTypeLlmV1Chat                  RouteType = "llm/v1/chat"
	RouteTypeLlmV1Completions           RouteType = "llm/v1/completions"
	RouteTypeLlmV1Embeddings            RouteType = "llm/v1/embeddings"
	RouteTypeLlmV1Files                 RouteType = "llm/v1/files"
	RouteTypeLlmV1Responses             RouteType = "llm/v1/responses"
	RouteTypePreserve                   RouteType = "preserve"
	RouteTypeRealtimeV1Realtime         RouteType = "realtime/v1/realtime"
)

func (e RouteType) ToPointer() *RouteType {
	return &e
}
func (e *RouteType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "audio/v1/audio/speech":
		fallthrough
	case "audio/v1/audio/transcriptions":
		fallthrough
	case "audio/v1/audio/translations":
		fallthrough
	case "image/v1/images/edits":
		fallthrough
	case "image/v1/images/generations":
		fallthrough
	case "llm/v1/assistants":
		fallthrough
	case "llm/v1/batches":
		fallthrough
	case "llm/v1/chat":
		fallthrough
	case "llm/v1/completions":
		fallthrough
	case "llm/v1/embeddings":
		fallthrough
	case "llm/v1/files":
		fallthrough
	case "llm/v1/responses":
		fallthrough
	case "preserve":
		fallthrough
	case "realtime/v1/realtime":
		*e = RouteType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RouteType: %v", v)
	}
}

type AiProxyPluginConfig struct {
	Auth *Auth `json:"auth,omitempty"`
	// Generative AI category of the request
	GenaiCategory *AiProxyPluginGenaiCategory `default:"text/generation" json:"genai_category"`
	// LLM input and output format and schema to use
	LlmFormat *AiProxyPluginLlmFormat `default:"openai" json:"llm_format"`
	Logging   *Logging                `json:"logging,omitempty"`
	// max allowed body size allowed to be introspected. 0 means unlimited, but the size of this body will still be limited by Nginx's client_max_body_size.
	MaxRequestBodySize *int64 `default:"8192" json:"max_request_body_size"`
	Model              Model  `json:"model"`
	// Display the model name selected in the X-Kong-LLM-Model response header
	ModelNameHeader *bool `default:"true" json:"model_name_header"`
	// Whether to 'optionally allow', 'deny', or 'always' (force) the streaming of answers via server sent events.
	ResponseStreaming *ResponseStreaming `default:"allow" json:"response_streaming"`
	// The model's operation implementation, for this provider.
	RouteType RouteType `json:"route_type"`
}

func (a AiProxyPluginConfig) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AiProxyPluginConfig) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AiProxyPluginConfig) GetAuth() *Auth {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *AiProxyPluginConfig) GetGenaiCategory() *AiProxyPluginGenaiCategory {
	if o == nil {
		return nil
	}
	return o.GenaiCategory
}

func (o *AiProxyPluginConfig) GetLlmFormat() *AiProxyPluginLlmFormat {
	if o == nil {
		return nil
	}
	return o.LlmFormat
}

func (o *AiProxyPluginConfig) GetLogging() *Logging {
	if o == nil {
		return nil
	}
	return o.Logging
}

func (o *AiProxyPluginConfig) GetMaxRequestBodySize() *int64 {
	if o == nil {
		return nil
	}
	return o.MaxRequestBodySize
}

func (o *AiProxyPluginConfig) GetModel() Model {
	if o == nil {
		return Model{}
	}
	return o.Model
}

func (o *AiProxyPluginConfig) GetModelNameHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ModelNameHeader
}

func (o *AiProxyPluginConfig) GetResponseStreaming() *ResponseStreaming {
	if o == nil {
		return nil
	}
	return o.ResponseStreaming
}

func (o *AiProxyPluginConfig) GetRouteType() RouteType {
	if o == nil {
		return RouteType("")
	}
	return o.RouteType
}

// AiProxyPluginConsumer - If set, the plugin will activate only for requests where the specified has been authenticated. (Note that some plugins can not be restricted to consumers this way.). Leave unset for the plugin to activate regardless of the authenticated Consumer.
type AiProxyPluginConsumer struct {
	ID *string `json:"id,omitempty"`
}

func (o *AiProxyPluginConsumer) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

// AiProxyPluginConsumerGroup - If set, the plugin will activate only for requests where the specified consumer group has been authenticated. (Note that some plugins can not be restricted to consumers groups this way.). Leave unset for the plugin to activate regardless of the authenticated Consumer Groups
type AiProxyPluginConsumerGroup struct {
	ID *string `json:"id,omitempty"`
}

func (o *AiProxyPluginConsumerGroup) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

type AiProxyPluginProtocols string

const (
	AiProxyPluginProtocolsGrpc  AiProxyPluginProtocols = "grpc"
	AiProxyPluginProtocolsGrpcs AiProxyPluginProtocols = "grpcs"
	AiProxyPluginProtocolsHTTP  AiProxyPluginProtocols = "http"
	AiProxyPluginProtocolsHTTPS AiProxyPluginProtocols = "https"
	AiProxyPluginProtocolsWs    AiProxyPluginProtocols = "ws"
	AiProxyPluginProtocolsWss   AiProxyPluginProtocols = "wss"
)

func (e AiProxyPluginProtocols) ToPointer() *AiProxyPluginProtocols {
	return &e
}
func (e *AiProxyPluginProtocols) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grpc":
		fallthrough
	case "grpcs":
		fallthrough
	case "http":
		fallthrough
	case "https":
		fallthrough
	case "ws":
		fallthrough
	case "wss":
		*e = AiProxyPluginProtocols(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AiProxyPluginProtocols: %v", v)
	}
}

// AiProxyPluginRoute - If set, the plugin will only activate when receiving requests via the specified route. Leave unset for the plugin to activate regardless of the route being used.
type AiProxyPluginRoute struct {
	ID *string `json:"id,omitempty"`
}

func (o *AiProxyPluginRoute) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

// AiProxyPluginService - If set, the plugin will only activate when receiving requests via one of the routes belonging to the specified Service. Leave unset for the plugin to activate regardless of the Service being matched.
type AiProxyPluginService struct {
	ID *string `json:"id,omitempty"`
}

func (o *AiProxyPluginService) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

// AiProxyPlugin - A Plugin entity represents a plugin configuration that will be executed during the HTTP request/response lifecycle. It is how you can add functionalities to Services that run behind Kong, like Authentication or Rate Limiting for example. You can find more information about how to install and what values each plugin takes by visiting the [Kong Hub](https://docs.konghq.com/hub/). When adding a Plugin Configuration to a Service, every request made by a client to that Service will run said Plugin. If a Plugin needs to be tuned to different values for some specific Consumers, you can do so by creating a separate plugin instance that specifies both the Service and the Consumer, through the `service` and `consumer` fields.
type AiProxyPlugin struct {
	// Unix epoch when the resource was created.
	CreatedAt *int64 `json:"created_at,omitempty"`
	// Whether the plugin is applied.
	Enabled *bool `default:"true" json:"enabled"`
	// A string representing a UUID (universally unique identifier).
	ID *string `json:"id,omitempty"`
	// A unique string representing a UTF-8 encoded name.
	InstanceName *string                `default:"null" json:"instance_name"`
	name         *string                `const:"ai-proxy" json:"name"`
	Ordering     *AiProxyPluginOrdering `json:"ordering"`
	// A list of partials to be used by the plugin.
	Partials []AiProxyPluginPartials `json:"partials"`
	// An optional set of strings associated with the Plugin for grouping and filtering.
	Tags []string `json:"tags"`
	// Unix epoch when the resource was last updated.
	UpdatedAt *int64              `json:"updated_at,omitempty"`
	Config    AiProxyPluginConfig `json:"config"`
	// If set, the plugin will activate only for requests where the specified has been authenticated. (Note that some plugins can not be restricted to consumers this way.). Leave unset for the plugin to activate regardless of the authenticated Consumer.
	Consumer *AiProxyPluginConsumer `json:"consumer"`
	// If set, the plugin will activate only for requests where the specified consumer group has been authenticated. (Note that some plugins can not be restricted to consumers groups this way.). Leave unset for the plugin to activate regardless of the authenticated Consumer Groups
	ConsumerGroup *AiProxyPluginConsumerGroup `json:"consumer_group"`
	// A list of the request protocols that will trigger this plugin. The default value, as well as the possible values allowed on this field, may change depending on the plugin type. For example, plugins that only work in stream mode will only support tcp and tls.
	Protocols []AiProxyPluginProtocols `json:"protocols"`
	// If set, the plugin will only activate when receiving requests via the specified route. Leave unset for the plugin to activate regardless of the route being used.
	Route *AiProxyPluginRoute `json:"route"`
	// If set, the plugin will only activate when receiving requests via one of the routes belonging to the specified Service. Leave unset for the plugin to activate regardless of the Service being matched.
	Service *AiProxyPluginService `json:"service"`
}

func (a AiProxyPlugin) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AiProxyPlugin) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AiProxyPlugin) GetCreatedAt() *int64 {
	if o == nil {
		return nil
	}
	return o.CreatedAt
}

func (o *AiProxyPlugin) GetEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.Enabled
}

func (o *AiProxyPlugin) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *AiProxyPlugin) GetInstanceName() *string {
	if o == nil {
		return nil
	}
	return o.InstanceName
}

func (o *AiProxyPlugin) GetName() *string {
	return types.String("ai-proxy")
}

func (o *AiProxyPlugin) GetOrdering() *AiProxyPluginOrdering {
	if o == nil {
		return nil
	}
	return o.Ordering
}

func (o *AiProxyPlugin) GetPartials() []AiProxyPluginPartials {
	if o == nil {
		return nil
	}
	return o.Partials
}

func (o *AiProxyPlugin) GetTags() []string {
	if o == nil {
		return nil
	}
	return o.Tags
}

func (o *AiProxyPlugin) GetUpdatedAt() *int64 {
	if o == nil {
		return nil
	}
	return o.UpdatedAt
}

func (o *AiProxyPlugin) GetConfig() AiProxyPluginConfig {
	if o == nil {
		return AiProxyPluginConfig{}
	}
	return o.Config
}

func (o *AiProxyPlugin) GetConsumer() *AiProxyPluginConsumer {
	if o == nil {
		return nil
	}
	return o.Consumer
}

func (o *AiProxyPlugin) GetConsumerGroup() *AiProxyPluginConsumerGroup {
	if o == nil {
		return nil
	}
	return o.ConsumerGroup
}

func (o *AiProxyPlugin) GetProtocols() []AiProxyPluginProtocols {
	if o == nil {
		return nil
	}
	return o.Protocols
}

func (o *AiProxyPlugin) GetRoute() *AiProxyPluginRoute {
	if o == nil {
		return nil
	}
	return o.Route
}

func (o *AiProxyPlugin) GetService() *AiProxyPluginService {
	if o == nil {
		return nil
	}
	return o.Service
}
