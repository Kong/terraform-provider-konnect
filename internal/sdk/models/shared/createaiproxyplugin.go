// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"fmt"
	"github.com/kong/terraform-provider-konnect/internal/sdk/internal/utils"
)

type CreateAIProxyPluginProtocols string

const (
	CreateAIProxyPluginProtocolsGrpc           CreateAIProxyPluginProtocols = "grpc"
	CreateAIProxyPluginProtocolsGrpcs          CreateAIProxyPluginProtocols = "grpcs"
	CreateAIProxyPluginProtocolsHTTP           CreateAIProxyPluginProtocols = "http"
	CreateAIProxyPluginProtocolsHTTPS          CreateAIProxyPluginProtocols = "https"
	CreateAIProxyPluginProtocolsTCP            CreateAIProxyPluginProtocols = "tcp"
	CreateAIProxyPluginProtocolsTLS            CreateAIProxyPluginProtocols = "tls"
	CreateAIProxyPluginProtocolsTLSPassthrough CreateAIProxyPluginProtocols = "tls_passthrough"
	CreateAIProxyPluginProtocolsUDP            CreateAIProxyPluginProtocols = "udp"
	CreateAIProxyPluginProtocolsWs             CreateAIProxyPluginProtocols = "ws"
	CreateAIProxyPluginProtocolsWss            CreateAIProxyPluginProtocols = "wss"
)

func (e CreateAIProxyPluginProtocols) ToPointer() *CreateAIProxyPluginProtocols {
	return &e
}

func (e *CreateAIProxyPluginProtocols) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grpc":
		fallthrough
	case "grpcs":
		fallthrough
	case "http":
		fallthrough
	case "https":
		fallthrough
	case "tcp":
		fallthrough
	case "tls":
		fallthrough
	case "tls_passthrough":
		fallthrough
	case "udp":
		fallthrough
	case "ws":
		fallthrough
	case "wss":
		*e = CreateAIProxyPluginProtocols(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateAIProxyPluginProtocols: %v", v)
	}
}

// CreateAIProxyPluginConsumer - If set, the plugin will activate only for requests where the specified has been authenticated. (Note that some plugins can not be restricted to consumers this way.). Leave unset for the plugin to activate regardless of the authenticated Consumer.
type CreateAIProxyPluginConsumer struct {
	ID *string `json:"id,omitempty"`
}

func (o *CreateAIProxyPluginConsumer) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

// CreateAIProxyPluginRoute - If set, the plugin will only activate when receiving requests via the specified route. Leave unset for the plugin to activate regardless of the Route being used.
type CreateAIProxyPluginRoute struct {
	ID *string `json:"id,omitempty"`
}

func (o *CreateAIProxyPluginRoute) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

// CreateAIProxyPluginService - If set, the plugin will only activate when receiving requests via one of the routes belonging to the specified Service. Leave unset for the plugin to activate regardless of the Service being matched.
type CreateAIProxyPluginService struct {
	ID *string `json:"id,omitempty"`
}

func (o *CreateAIProxyPluginService) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

// RouteType - The model's operation implementation, for this provider.
type RouteType string

const (
	RouteTypeLlmV1Chat        RouteType = "llm/v1/chat"
	RouteTypeLlmV1Completions RouteType = "llm/v1/completions"
)

func (e RouteType) ToPointer() *RouteType {
	return &e
}

func (e *RouteType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "llm/v1/chat":
		fallthrough
	case "llm/v1/completions":
		*e = RouteType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RouteType: %v", v)
	}
}

// ParamLocation - Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.
type ParamLocation string

const (
	ParamLocationQuery ParamLocation = "query"
	ParamLocationBody  ParamLocation = "body"
)

func (e ParamLocation) ToPointer() *ParamLocation {
	return &e
}

func (e *ParamLocation) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "query":
		fallthrough
	case "body":
		*e = ParamLocation(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParamLocation: %v", v)
	}
}

type Auth struct {
	// If AI model requires authentication via Authorization or API key header, specify its name here.
	HeaderName *string `json:"header_name,omitempty"`
	// Specify the full auth header value for 'header_name', for example 'Bearer key' or just 'key'.
	HeaderValue *string `json:"header_value,omitempty"`
	// If AI model requires authentication via query parameter, specify its name here.
	ParamName *string `json:"param_name,omitempty"`
	// Specify the full parameter value for 'param_name'.
	ParamValue *string `json:"param_value,omitempty"`
	// Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.
	ParamLocation *ParamLocation `json:"param_location,omitempty"`
}

func (o *Auth) GetHeaderName() *string {
	if o == nil {
		return nil
	}
	return o.HeaderName
}

func (o *Auth) GetHeaderValue() *string {
	if o == nil {
		return nil
	}
	return o.HeaderValue
}

func (o *Auth) GetParamName() *string {
	if o == nil {
		return nil
	}
	return o.ParamName
}

func (o *Auth) GetParamValue() *string {
	if o == nil {
		return nil
	}
	return o.ParamValue
}

func (o *Auth) GetParamLocation() *ParamLocation {
	if o == nil {
		return nil
	}
	return o.ParamLocation
}

// Provider - AI provider request format - Kong translates requests to and from the specified backend compatible formats.
type Provider string

const (
	ProviderOpenai    Provider = "openai"
	ProviderAzure     Provider = "azure"
	ProviderAnthropic Provider = "anthropic"
	ProviderCohere    Provider = "cohere"
	ProviderMistral   Provider = "mistral"
	ProviderLlama2    Provider = "llama2"
)

func (e Provider) ToPointer() *Provider {
	return &e
}

func (e *Provider) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "openai":
		fallthrough
	case "azure":
		fallthrough
	case "anthropic":
		fallthrough
	case "cohere":
		fallthrough
	case "mistral":
		fallthrough
	case "llama2":
		*e = Provider(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Provider: %v", v)
	}
}

// Llama2Format - If using llama2 provider, select the upstream message format.
type Llama2Format string

const (
	Llama2FormatRaw    Llama2Format = "raw"
	Llama2FormatOpenai Llama2Format = "openai"
	Llama2FormatOllama Llama2Format = "ollama"
)

func (e Llama2Format) ToPointer() *Llama2Format {
	return &e
}

func (e *Llama2Format) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "raw":
		fallthrough
	case "openai":
		fallthrough
	case "ollama":
		*e = Llama2Format(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Llama2Format: %v", v)
	}
}

// MistralFormat - If using mistral provider, select the upstream message format.
type MistralFormat string

const (
	MistralFormatOpenai MistralFormat = "openai"
	MistralFormatOllama MistralFormat = "ollama"
)

func (e MistralFormat) ToPointer() *MistralFormat {
	return &e
}

func (e *MistralFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "openai":
		fallthrough
	case "ollama":
		*e = MistralFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MistralFormat: %v", v)
	}
}

// Options - Key/value settings for the model
type Options struct {
	// Defines the max_tokens, if using chat or completion models.
	MaxTokens *int64 `default:"256" json:"max_tokens"`
	// Defines the matching temperature, if using chat or completion models.
	Temperature *float64 `default:"1" json:"temperature"`
	// Defines the top-p probability mass, if supported.
	TopP *float64 `default:"1" json:"top_p"`
	// Defines the top-k most likely tokens, if supported.
	TopK *int64 `default:"0" json:"top_k"`
	// Defines the schema/API version, if using Anthropic provider.
	AnthropicVersion *string `json:"anthropic_version,omitempty"`
	// Instance name for Azure OpenAI hosted models.
	AzureInstance *string `json:"azure_instance,omitempty"`
	// 'api-version' for Azure OpenAI instances.
	AzureAPIVersion *string `default:"2023-05-15" json:"azure_api_version"`
	// Deployment ID for Azure OpenAI instances.
	AzureDeploymentID *string `json:"azure_deployment_id,omitempty"`
	// If using llama2 provider, select the upstream message format.
	Llama2Format *Llama2Format `json:"llama2_format,omitempty"`
	// If using mistral provider, select the upstream message format.
	MistralFormat *MistralFormat `json:"mistral_format,omitempty"`
	// Manually specify or override the full URL to the AI operation endpoints, when calling (self-)hosted models, or for running via a private endpoint.
	UpstreamURL *string `json:"upstream_url,omitempty"`
}

func (o Options) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *Options) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Options) GetMaxTokens() *int64 {
	if o == nil {
		return nil
	}
	return o.MaxTokens
}

func (o *Options) GetTemperature() *float64 {
	if o == nil {
		return nil
	}
	return o.Temperature
}

func (o *Options) GetTopP() *float64 {
	if o == nil {
		return nil
	}
	return o.TopP
}

func (o *Options) GetTopK() *int64 {
	if o == nil {
		return nil
	}
	return o.TopK
}

func (o *Options) GetAnthropicVersion() *string {
	if o == nil {
		return nil
	}
	return o.AnthropicVersion
}

func (o *Options) GetAzureInstance() *string {
	if o == nil {
		return nil
	}
	return o.AzureInstance
}

func (o *Options) GetAzureAPIVersion() *string {
	if o == nil {
		return nil
	}
	return o.AzureAPIVersion
}

func (o *Options) GetAzureDeploymentID() *string {
	if o == nil {
		return nil
	}
	return o.AzureDeploymentID
}

func (o *Options) GetLlama2Format() *Llama2Format {
	if o == nil {
		return nil
	}
	return o.Llama2Format
}

func (o *Options) GetMistralFormat() *MistralFormat {
	if o == nil {
		return nil
	}
	return o.MistralFormat
}

func (o *Options) GetUpstreamURL() *string {
	if o == nil {
		return nil
	}
	return o.UpstreamURL
}

type Model struct {
	// AI provider request format - Kong translates requests to and from the specified backend compatible formats.
	Provider *Provider `json:"provider,omitempty"`
	// Model name to execute.
	Name *string `json:"name,omitempty"`
	// Key/value settings for the model
	Options *Options `json:"options,omitempty"`
}

func (o *Model) GetProvider() *Provider {
	if o == nil {
		return nil
	}
	return o.Provider
}

func (o *Model) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *Model) GetOptions() *Options {
	if o == nil {
		return nil
	}
	return o.Options
}

type Logging struct {
	// If enabled and supported by the driver, will add model usage and token metrics into the Kong log plugin(s) output.
	LogStatistics *bool `default:"true" json:"log_statistics"`
	// If enabled, will log the request and response body into the Kong log plugin(s) output.
	LogPayloads *bool `default:"false" json:"log_payloads"`
}

func (l Logging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *Logging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Logging) GetLogStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.LogStatistics
}

func (o *Logging) GetLogPayloads() *bool {
	if o == nil {
		return nil
	}
	return o.LogPayloads
}

type CreateAIProxyPluginConfig struct {
	// The model's operation implementation, for this provider.
	RouteType *RouteType `json:"route_type,omitempty"`
	Auth      *Auth      `json:"auth,omitempty"`
	Model     Model      `json:"model"`
	Logging   *Logging   `json:"logging,omitempty"`
}

func (o *CreateAIProxyPluginConfig) GetRouteType() *RouteType {
	if o == nil {
		return nil
	}
	return o.RouteType
}

func (o *CreateAIProxyPluginConfig) GetAuth() *Auth {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *CreateAIProxyPluginConfig) GetModel() Model {
	if o == nil {
		return Model{}
	}
	return o.Model
}

func (o *CreateAIProxyPluginConfig) GetLogging() *Logging {
	if o == nil {
		return nil
	}
	return o.Logging
}

// CreateAIProxyPlugin - A Plugin entity represents a plugin configuration that will be executed during the HTTP request/response lifecycle. It is how you can add functionalities to Services that run behind Kong, like Authentication or Rate Limiting for example. You can find more information about how to install and what values each plugin takes by visiting the [Kong Hub](https://docs.konghq.com/hub/). When adding a Plugin Configuration to a Service, every request made by a client to that Service will run said Plugin. If a Plugin needs to be tuned to different values for some specific Consumers, you can do so by creating a separate plugin instance that specifies both the Service and the Consumer, through the `service` and `consumer` fields.
type CreateAIProxyPlugin struct {
	// Whether the plugin is applied.
	Enabled *bool  `default:"true" json:"enabled"`
	name    string `const:"ai-proxy" json:"name"`
	// A list of the request protocols that will trigger this plugin. The default value, as well as the possible values allowed on this field, may change depending on the plugin type. For example, plugins that only work in stream mode will only support `"tcp"` and `"tls"`.
	Protocols []CreateAIProxyPluginProtocols `json:"protocols,omitempty"`
	// An optional set of strings associated with the Plugin for grouping and filtering.
	Tags []string `json:"tags,omitempty"`
	// If set, the plugin will activate only for requests where the specified has been authenticated. (Note that some plugins can not be restricted to consumers this way.). Leave unset for the plugin to activate regardless of the authenticated Consumer.
	Consumer *CreateAIProxyPluginConsumer `json:"consumer,omitempty"`
	// If set, the plugin will only activate when receiving requests via the specified route. Leave unset for the plugin to activate regardless of the Route being used.
	Route *CreateAIProxyPluginRoute `json:"route,omitempty"`
	// If set, the plugin will only activate when receiving requests via one of the routes belonging to the specified Service. Leave unset for the plugin to activate regardless of the Service being matched.
	Service *CreateAIProxyPluginService `json:"service,omitempty"`
	Config  CreateAIProxyPluginConfig   `json:"config"`
}

func (c CreateAIProxyPlugin) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateAIProxyPlugin) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateAIProxyPlugin) GetEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.Enabled
}

func (o *CreateAIProxyPlugin) GetName() string {
	return "ai-proxy"
}

func (o *CreateAIProxyPlugin) GetProtocols() []CreateAIProxyPluginProtocols {
	if o == nil {
		return nil
	}
	return o.Protocols
}

func (o *CreateAIProxyPlugin) GetTags() []string {
	if o == nil {
		return nil
	}
	return o.Tags
}

func (o *CreateAIProxyPlugin) GetConsumer() *CreateAIProxyPluginConsumer {
	if o == nil {
		return nil
	}
	return o.Consumer
}

func (o *CreateAIProxyPlugin) GetRoute() *CreateAIProxyPluginRoute {
	if o == nil {
		return nil
	}
	return o.Route
}

func (o *CreateAIProxyPlugin) GetService() *CreateAIProxyPluginService {
	if o == nil {
		return nil
	}
	return o.Service
}

func (o *CreateAIProxyPlugin) GetConfig() CreateAIProxyPluginConfig {
	if o == nil {
		return CreateAIProxyPluginConfig{}
	}
	return o.Config
}
